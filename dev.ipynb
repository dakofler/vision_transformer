{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\n",
    "\n",
    "Daniel Kofler 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "patch_size = 4\n",
    "num_patches = (32 // patch_size)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision_transformer import VisionTransformer\n",
    "\n",
    "vit = VisionTransformer(\n",
    "    in_channels=3,\n",
    "    num_patches=num_patches,\n",
    "    patch_size=patch_size,\n",
    "    embed_dim=384,\n",
    "    num_heads=6,\n",
    "    num_layers=6,\n",
    "    num_classes=10,\n",
    "    dropout=0.3\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(vit, (3, 32, 32), batch_size=batch_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, \"rb\") as fo:\n",
    "        dict = pickle.load(fo, encoding=\"bytes\")\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = [\"data/data_batch_1\", \"data/data_batch_2\", \"data/data_batch_3\", \"data/data_batch_4\", \"data/data_batch_5\"]\n",
    "X_train = torch.concat([torch.tensor(unpickle(p)[b'data']) for p in train_batches], dim=0).view(-1, 3, 32, 32) / 255.0\n",
    "y_train = torch.concat([torch.tensor(unpickle(p)[b'labels']) for p in train_batches], dim=0)\n",
    "X_test = torch.tensor(unpickle(\"data/test_batch\")[b'data']).view(-1, 3, 32, 32) / 255.0\n",
    "y_test = torch.tensor(unpickle(\"data/test_batch\")[b'labels'])\n",
    "\n",
    "labels = {\n",
    "    0: \"airplane\",\n",
    "    1: \"automobile\",\n",
    "    2: \"bird\",\n",
    "    3: \"cat\",\n",
    "    4: \"deer\",\n",
    "    5: \"dog\",\n",
    "    6: \"frog\",\n",
    "    7: \"horse\",\n",
    "    8: \"ship\",\n",
    "    9: \"truck\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    idx = randint(0, len(X_train))\n",
    "    plt.imshow(X_train[idx].permute(1, 2, 0))\n",
    "    plt.title(labels[y_train[idx].item()])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_idx = torch.randperm(len(X_train))\n",
    "X_train_shuffled, y_train_shuffled = X_train[rand_idx], y_train[rand_idx]\n",
    "n = int(0.8 * len(X_train))\n",
    "\n",
    "X_train = X_train_shuffled[:n]\n",
    "y_train = y_train_shuffled[:n]\n",
    "X_val = X_train_shuffled[n:]\n",
    "y_val = y_train_shuffled[n:]\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "lr = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "optim = torch.optim.AdamW(vit.parameters(), lr=lr)\n",
    "train_dl = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_steps = len(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(logits, target) -> float:\n",
    "    with torch.no_grad():\n",
    "        return (F.softmax(logits, dim=-1).argmax(-1) == target).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_interval = 25\n",
    "\n",
    "for e in range(1, epochs + 1):\n",
    "\n",
    "    # training\n",
    "    vit.train()\n",
    "    train_loss = train_accuracy = 0.0\n",
    "    for step, batch in enumerate(train_dl):\n",
    "        print(f\"step {step}/{train_steps}\", end=\"\\r\")\n",
    "        X = batch[0].to(device)\n",
    "        y = batch[1].to(device)\n",
    "\n",
    "        logits = vit(X)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        train_loss += loss.item()\n",
    "        train_accuracy += accuracy(logits, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "\n",
    "    train_loss /= len(train_dl)\n",
    "    train_accuracy /= len(train_dl)\n",
    "\n",
    "    # validation\n",
    "    vit.eval()\n",
    "    val_loss = val_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dl:\n",
    "            X = batch[0].to(device)\n",
    "            y = batch[1].to(device)\n",
    "            logits = vit(X)\n",
    "            val_loss += F.cross_entropy(logits, y).item()\n",
    "            val_accuracy += accuracy(logits, y)\n",
    "\n",
    "    val_loss /= len(val_dl)\n",
    "    val_accuracy /= len(val_dl)\n",
    "\n",
    "    # save checkpoints\n",
    "    if e > 1 and e % ckpt_interval == 0:\n",
    "        sd = {\n",
    "            \"model\": vit.state_dict(),\n",
    "            \"optim\": optim.state_dict(),\n",
    "            \"epoch\" : e\n",
    "        }\n",
    "        torch.save(sd, f\"vit_{e}.pt\")\n",
    "\n",
    "    print(f\"epoch {e}/{epochs} | train_loss {train_loss:.4f} | train_acc {train_accuracy:.4f} | val_loss {val_loss:.4f} | val_acc {val_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
